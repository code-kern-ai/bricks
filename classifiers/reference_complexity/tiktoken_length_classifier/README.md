Uses OpenAI's tiktoken tokenizer library to classify a text based on the amount of tokens in a text. The tokenizer is used for the GPT models and converts words into integers. The conversion is reversible and lossless, meaning that a tokenized sentence can be converted back. This brick counts the length of a tokenized text and classifies it as "short", "medium" or "long".